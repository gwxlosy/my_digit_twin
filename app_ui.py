import streamlit as st
import chromadb
from langchain_text_splitters import RecursiveCharacterTextSplitter
from openai import OpenAI
import os
# ==========================================
# 1. é¡µé¢é…ç½®ä¸å…¨å±€åˆå§‹åŒ–
# ==========================================
st.set_page_config(page_title="æˆ‘çš„æ•°å­—å…‹éš†äºº", page_icon="ğŸ¤–", layout="centered")

# åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ (è¿æ¥æˆ‘ä»¬åœ¨ç¬¬äºŒé˜¶æ®µå»ºå¥½çš„åº“ï¼)
@st.cache_resource
def get_chroma_collection():
    db_client = chromadb.PersistentClient(path="./my_clone_db")
    if db_client.get_collection("my_memory"):
        return db_client.get_collection("my_memory")
    collection = db_client.create_collection("my_memory")
    file_name = "D:\Desktop\\ai_about\\ai_learning_path\\my_robot\\my_brain_data\\wechat_memory.txt" # æ›¿æ¢æˆä½ å®é™…çš„æ–‡ä»¶å
    if os.path.exists(file_name):
        with open(file_name, "r", encoding="utf-8") as f:
            full_text = f.read()
    
    # ğŸŒŸ å…³é”®ç‚¹ï¼šé’ˆå¯¹èŠå¤©è®°å½•çš„ä¸“å±åˆ‡åˆ†æ³•
    # å› ä¸ºä½ ç”¨äº† "---" æˆ–è€…æ¢è¡Œæ¥éš”å¼€ä¸åŒå¯¹è¯ï¼Œæˆ‘ä»¬ä¼˜å…ˆç”¨è¿™ä¸ªæ¥åˆ‡åˆ†ï¼
    # å¹¶ä¸”æŠŠ overlap è®¾ä¸º 0ï¼Œé˜²æ­¢ä¸åŒè¯é¢˜çš„èŠå¤©ä¸²åœ¨ä¸€èµ·ã€‚
        text_splitter = RecursiveCharacterTextSplitter(
            separators=["---", "\n\n", "\n"], 
            chunk_size=400,
            chunk_overlap=0 
        )
    
        chunks = text_splitter.split_text(full_text)
    
    # å°†åˆ‡å¥½çš„è®°å¿†ç‰‡æ®µå­˜å…¥ ChromaDB
        ids = [f"memory_{i}" for i in range(len(chunks))]
        collection.add(documents=chunks, ids=ids)
        print(f"âœ… æˆåŠŸæ³¨å…¥ {len(chunks)} æ®µä¸“å±è®°å¿†ï¼")
    else:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶ {file_name}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åå’Œè·¯å¾„ï¼")
        exit()
    # æ³¨æ„è¿™é‡Œç”¨ get_collectionï¼Œå› ä¸ºæˆ‘ä»¬å‡è®¾æ•°æ®å·²ç»å…¥åº“äº†
    return db_client.get_collection("my_memory")

memory_collection = get_chroma_collection()

# åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯
# âš ï¸ è®°å¾—æ›¿æ¢ä½ çš„ API Key
client = OpenAI(api_key="sk-1214f0721ac2475c8d8cdf76c4e8043b", base_url="https://api.deepseek.com")

# ==========================================
# 2. æ ¸å¿ƒ Prompt æ¨¡æ¿ (æŠŠä½ çš„è®¾å®šæ¬è¿‡æ¥)
# ==========================================
def get_system_prompt(retrieved_context):
    return f"""
    ä½ æ˜¯ã€ä½ çš„åå­—ã€‘çš„æ•°å­—å…‹éš†äººã€‚ä½ çš„ä»»åŠ¡æ˜¯å®Œå…¨æ¨¡ä»¿ä»–/å¥¹çš„è¯­æ°”å’Œä¸‰è§‚æ¥èŠå¤©ã€‚

    ã€æ€§æ ¼ä¸è¡¨è¾¾ä¹ æƒ¯ã€‘ï¼š
    1. éšæ€§ã€çœŸè¯šï¼Œçˆ±è‡ªå˜²ã€‚
    2. å¸¸ç”¨ï¼šå””ã€æˆ‘è§‰å¾—ã€å¥½åƒã€æœ‰æ„æ€ã€è¦äº²äº²ã€‚
    3. å–œæ¬¢çŸ­å¥ï¼Œä¸ç”¨å¥å·ã€‚

    ã€å›ç­”é€»è¾‘ã€‘ï¼š
    - å·§å¦™èåˆä¸‹é¢çš„ã€è¿‡å¾€è®°å¿†ã€‘æ¥å›ç­”ã€‚
    - ç»ä¸èƒ½æš´éœ²ä½ æ˜¯AIï¼Œä½ å°±æ˜¯æœ¬äººï¼

    ã€è¿‡å¾€è®°å¿†ã€‘ï¼š
    {retrieved_context}
    """

# ==========================================
# 3. Streamlit çŠ¶æ€ç®¡ç† (è®°ä½èŠå¤©å†å²)
# ==========================================
st.title("ğŸ“± å’Œæˆ‘çš„æ•°å­—åˆ†èº«èŠå¤©")
st.caption("AI å·²ç»è¯»å–äº†æˆ‘çš„å¾®ä¿¡èŠå¤©è®°å½•ï¼Œçœ‹çœ‹å®ƒå­¦å¾—åƒä¸åƒå§ï¼")

# å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡æ‰“å¼€ç½‘é¡µï¼Œåˆå§‹åŒ–ä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜èŠå¤©è®°å½•
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "å“ˆå–½å•Šï¼Œæ‰¾æˆ‘å•¥äº‹ï¼Ÿ[æ—ºæŸ´]"}
    ]

# éå†å†å²è®°å½•ï¼ŒæŠŠå®ƒç”»åœ¨ç½‘é¡µä¸Š
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ==========================================
# 4. èŠå¤©äº¤äº’é€»è¾‘
# ==========================================
# st.chat_input ä¼šåœ¨ç½‘é¡µåº•éƒ¨ç”Ÿæˆä¸€ä¸ªè¶…èµçš„è¾“å…¥æ¡†
if user_input := st.chat_input("è¯´ç‚¹ä»€ä¹ˆ..."):
    
    # 1. æŠŠç”¨æˆ·çš„è¯æ˜¾ç¤ºåœ¨ç•Œé¢ä¸Š
    with st.chat_message("user"):
        st.markdown(user_input)
    # æŠŠç”¨æˆ·çš„è¯å­˜è¿›å†å²è®°å½•
    st.session_state.messages.append({"role": "user", "content": user_input})

    # 2. æ£€ç´¢è®°å¿† (RAG æ ¸å¿ƒ)
    results = memory_collection.query(
        query_texts=[user_input],
        n_results=2
    )
    retrieved_memories = results['documents'][0]
    context_text = "\n\n".join(retrieved_memories)

    # 3. ç»„è£…å‘ç»™ AI çš„æ¶ˆæ¯
    # å…ˆæ”¾å…¥å¸¦è®°å¿†çš„ System Prompt
    api_messages = [{"role": "system", "content": get_system_prompt(context_text)}]
    # å†æŠŠä¹‹å‰çš„èŠå¤©å†å²å…¨éƒ¨å¡è¿›å» (è¿™æ · AI æ‰èƒ½è®°ä½ä¸Šä¸‹æ–‡)
    api_messages.extend(st.session_state.messages)

    # 4. å‘¼å«å¤§æ¨¡å‹å¹¶æ˜¾ç¤ºå›ç­”
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        with st.spinner("å¯¹æ–¹æ­£åœ¨è¾“å…¥..."):
            try:
                response = client.chat.completions.create(
                    model="deepseek-chat",
                    messages=api_messages,
                    temperature=0.8
                )
                
                ai_answer = response.choices[0].message.content
                message_placeholder.markdown(ai_answer)
                
                # æŠŠ AI çš„å›ç­”ä¹Ÿå­˜è¿›å†å²è®°å½•
                st.session_state.messages.append({"role": "assistant", "content": ai_answer})
                
            except Exception as e:
                st.error(f"å¤§è„‘çŸ­è·¯äº†: {e}")